#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sincronizador DB2 -> SQLite para Sales Analytics Dashboard
Coleta dados do DB2 e salva no database.db local.
Modo INCREMENTAL: usa CHAVE única para evitar duplicatas.

Uso:
    python sync_db2.py                        # Sync incremental (última semana)
    python sync_db2.py --desde 2025-01-01     # Carga desde data específica
    python sync_db2.py --loop 600             # Sync a cada 10 minutos
    python sync_db2.py --loop 600 --serve     # Sync + servidor web
"""

import os
import sys
import time
import sqlite3
import argparse
import subprocess
import threading
import platform
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json

# === MODIFICATION: pyodbc mandatory ===
import pyodbc
# =====================================

# === PostgreSQL mapping support ===
try:
    import psycopg2
    import json as json_module
except ImportError:
    psycopg2 = None
# ==================================

# === CONFIGURAÇÃO ===
STRING_CONEXAO_DB2 = (
    "DSN=CISSODBC;UID=CONSULTA;PWD=qazwsx@123;"
    "MODE=SHARE;CLIENTENCALG=2;PROTOCOL=TCPIP;"
    "TXNISOLATION=1;SERVICENAME=50000;HOSTNAME=192.168.1.200;"
    "DATABASE=CISSERP;"
)

QUIET = False

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = SCRIPT_DIR
DATABASE_PATH = os.path.join(PROJECT_ROOT, "database.db")


def log(msg: str):
    """Log com timestamp completo YYYY-MM-DD HH:MM:SS."""
    if QUIET:
        return
    # Formato solicitado: [2026-02-08 21:30:13] Msg
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

# ... (skipping to main)

def main_unused():
    global QUIET
    parser = argparse.ArgumentParser(description="Sincronizador DB2 -> SQLite")
    parser.add_argument("--desde", type=str, metavar="YYYY-MM-DD",
                        help="Ignorado (Janela fixa 31 dias)")
    parser.add_argument("--loop", type=int, metavar="SEGUNDOS",
                        help="Intervalo do loop")
    parser.add_argument("--serve", action="store_true",
                        help="Inicia o servidor web após sync")
    parser.add_argument("--quiet", action="store_true",
                        help="Suprime logs no stdout")
    
    args = parser.parse_args()
    
    if args.quiet:
        QUIET = True
    
    # 1. Sincronização Inicial (Bloqueante)
    # Ex: [2026-02-08 21:30:13] Sync iniciado | modo=serve | SO=Windows
    modo_str = "serve" if args.serve else ("loop" if args.loop else "once")
    if not QUIET:
        log(f"Sync iniciado | modo={modo_str} | SO={platform.system()}")

    # Garantir que tabelas existam
    inicializar_sqlite()
    
    sucesso = sincronizar(data_inicial=args.desde)
    
    # 2. Configurar Loop (Thread se Serve, Main se Loop-Only)
    if args.loop:
        intervalo = args.loop
        
        def loop_sync_internal(): 
            while True:
                time.sleep(intervalo)
                sincronizar()
        
        if args.serve:
            t = threading.Thread(target=loop_sync_internal, daemon=True)
            t.start()
        else:
            try:
                loop_sync_internal()
            except KeyboardInterrupt:
                pass

    # 3. Servidor Web
    if args.serve:
        iniciar_servidor()


def conectar_db2():
    """Conecta ao DB2."""
    conn = pyodbc.connect(STRING_CONEXAO_DB2, timeout=30)
    log("DB2 OK | conexão estabelecida")
    return conn


def executar_sql_db2(conn, query: str) -> List[Dict[str, Any]]:
    """Executa SQL no DB2 e retorna lista de dicionários."""
    cursor = conn.cursor()
    try:
        # Define o schema antes de executar a query
        cursor.execute("SET CURRENT SCHEMA DBA")
        cursor.execute(query)
    except Exception as e:
        log(f"  ERRO SQL: {e}")
        log(f"  Query (primeiros 500 chars): {query[:500]}...")
        return []
    
    if cursor.description is None:
        return []
    
    colunas = [col[0].strip() for col in cursor.description]
    rows = cursor.fetchall()
    return [dict(zip(colunas, row)) for row in rows]


def formatar_data(valor) -> str:
    """Formata data para YYYY-MM-DD."""
    if valor is None:
        return ""
    if hasattr(valor, 'strftime'):
        return valor.strftime('%Y-%m-%d')
    return str(valor)[:10]


def formatar_datetime(valor) -> str:
    """Formata datetime para ISO 8601 completo (YYYY-MM-DDTHH:MM:SS)."""
    if valor is None:
        return ""
    if hasattr(valor, 'strftime'):
        return valor.strftime('%Y-%m-%dT%H:%M:%S')
    # Se vier como string "2025-02-08 21:50:00", converter para formato ISO
    valor_str = str(valor)
    if ' ' in valor_str:
        # Substituir espaço por 'T' para formato ISO
        return valor_str.replace(' ', 'T')[:19]
    return valor_str[:19]  # Retorna até segundos


def formatar_hora(valor) -> str:
    """Formata hora para HH:MM:SS."""
    if valor is None:
        return ""
    if hasattr(valor, 'strftime'):
        return valor.strftime('%H:%M:%S')
    return str(valor)[:8]


def inicializar_sqlite():
    """Inicializa o banco SQLite com o schema."""
    log(f"Inicializando SQLite em {DATABASE_PATH}...")
    
    try:
        conn = sqlite3.connect(DATABASE_PATH, timeout=10.0)
        cursor = conn.cursor()
        
        # Enable WAL mode and set busy timeout for concurrent access
        cursor.execute("PRAGMA journal_mode = WAL")
        cursor.execute("PRAGMA busy_timeout = 5000")
        conn.commit()
        
        # 1. Cache Orcamentos
        try:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS cache_orcamentos (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    CHAVE TEXT UNIQUE NOT NULL,
                    IDEMPRESA INTEGER,
                    IDORCAMENTO INTEGER,
                    IDPRODUTO TEXT,
                    IDSUBPRODUTO TEXT,
                    NUMSEQUENCIA INTEGER,
                    QTDPRODUTO REAL,
                    UNIDADE TEXT,
                    FABRICANTE TEXT,
                    VALUNITBRUTO REAL,
                    VALTOTLIQUIDO REAL,
                    DESCRRESPRODUTO TEXT,
                    IDVENDEDOR TEXT,
                    IDLOCALRETIRADA INTEGER,
                    IDSECAO INTEGER,
                    DESCRSECAO TEXT,
                    TIPOENTREGA TEXT,
                    NOMEVENDEDOR TEXT,
                    TIPOENTREGA_DESCR TEXT,
                    LOCALRETESTOQUE TEXT,
                    FLAGCANCELADO TEXT,
                    IDCLIFOR TEXT,
                    DESCLIENTE TEXT,
                    DTMOVIMENTO TEXT,
                    IDRECEBIMENTO TEXT,
                    DESCRRECEBIMENTO TEXT,
                    FLAGPRENOTAPAGA TEXT,
                    CODBARRAS TEXT,
                    CODBARRAS_CAIXA TEXT,
                    sync_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
        except Exception as e:
            log(f"Erro ao criar cache_orcamentos: {e}")

        # 2. Remover tabelas antigas
        try:
            cursor.execute("DROP TABLE IF EXISTS cache_vendas_pendentes")
            cursor.execute("DROP TABLE IF EXISTS cache_tubos_conexoes")
            conn.commit()
        except Exception as e:
            log(f"Erro ao limpar tabelas antigas: {e}")

        # 3. Companies & Goals & Alerts & Pickup Points
        try:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS pickup_points (
                    id INTEGER PRIMARY KEY,
                    name TEXT NOT NULL,
                    active INTEGER DEFAULT 1
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS companies (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    cnpj TEXT UNIQUE NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS goals (
                    id TEXT PRIMARY KEY,
                    salesperson_id TEXT NOT NULL,
                    company_id TEXT NOT NULL,
                    type TEXT NOT NULL,
                    target_value REAL NOT NULL,
                    month INTEGER NOT NULL,
                    year INTEGER NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS alerts (
                    id TEXT PRIMARY KEY,
                    company_id TEXT NOT NULL,
                    type TEXT NOT NULL,
                    title TEXT NOT NULL,
                    message TEXT NOT NULL,
                    salesperson_id TEXT,
                    severity TEXT DEFAULT 'warning',
                    is_read INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
        except Exception as e:
            log(f"Erro ao criar tabelas comp/goals/alerts: {e}")
            
        # 4. Indices
        try:
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_orc_dt ON cache_orcamentos(DTMOVIMENTO)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_orc_vend ON cache_orcamentos(IDVENDEDOR)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_orc_chave ON cache_orcamentos(CHAVE)")
            conn.commit()
        except Exception as e:
            log(f"Erro ao criar indices: {e}")

        # 5. Users
        try:
            # Check if settings column exists if table exists
            cursor.execute("PRAGMA table_info(users)")
            columns = [info[1] for info in cursor.fetchall()]
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id TEXT PRIMARY KEY,
                    username TEXT UNIQUE NOT NULL,
                    password TEXT NOT NULL,
                    name TEXT NOT NULL,
                    role TEXT NOT NULL,
                    sections TEXT,
                    settings TEXT,
                    active INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            if 'settings' not in columns and 'users' in [t[0] for t in cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")]:
                 # Only try to alter if table exists but column doesn't
                 try:
                     cursor.execute("ALTER TABLE users ADD COLUMN settings TEXT")
                 except:
                     pass
            conn.commit()
        except Exception as e:
            log(f"Erro ao criar users: {e}")
            
        # 6. Sections
        try:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS sections (
                    id INTEGER PRIMARY KEY,
                    name TEXT NOT NULL
                )
            """)
            conn.commit()
        except Exception as e:
            log(f"Erro ao criar sections: {e}")
            
        # 7. App Tables
        try:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS products (
                    id TEXT PRIMARY KEY,
                    erp_code TEXT UNIQUE NOT NULL,
                    barcode TEXT,
                    box_barcode TEXT,
                    name TEXT NOT NULL,
                    section TEXT NOT NULL,
                    pickup_point INTEGER NOT NULL,
                    unit TEXT DEFAULT 'UN' NOT NULL,
                    manufacturer TEXT,
                    price REAL DEFAULT 0 NOT NULL,
                    stock_qty REAL DEFAULT 0 NOT NULL,
                    erp_updated_at TEXT
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS routes (
                    id TEXT PRIMARY KEY,
                    code TEXT UNIQUE NOT NULL,
                    name TEXT NOT NULL,
                    description TEXT,
                    active INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS orders (
                    id TEXT PRIMARY KEY,
                    erp_order_id TEXT UNIQUE NOT NULL,
                    customer_name TEXT NOT NULL,
                    customer_code TEXT,
                    total_value REAL DEFAULT 0 NOT NULL,
                    observation TEXT,
                    status TEXT DEFAULT 'pendente' NOT NULL,
                    financial_status TEXT DEFAULT 'pendente' NOT NULL,
                    priority INTEGER DEFAULT 0 NOT NULL,
                    is_launched INTEGER DEFAULT 0 NOT NULL,
                    route_id TEXT REFERENCES routes(id),
                    separation_code TEXT UNIQUE,
                    pickup_points TEXT,
                    erp_updated_at TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS order_items (
                    id TEXT PRIMARY KEY,
                    order_id TEXT NOT NULL REFERENCES orders(id),
                    product_id TEXT NOT NULL REFERENCES products(id),
                    quantity REAL NOT NULL,
                    separated_qty REAL DEFAULT 0 NOT NULL,
                    checked_qty REAL DEFAULT 0 NOT NULL,
                    status TEXT DEFAULT 'pendente' NOT NULL,
                    pickup_point INTEGER NOT NULL,
                    section TEXT NOT NULL,
                    qty_picked REAL DEFAULT 0,
                    qty_checked REAL DEFAULT 0,
                    exception_type TEXT
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS work_units (
                    id TEXT PRIMARY KEY,
                    order_id TEXT REFERENCES orders(id),
                    status TEXT NOT NULL,
                    type TEXT NOT NULL,
                    pickup_point INTEGER,
                    section TEXT,
                    assigned_user_id TEXT REFERENCES users(id),
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    completed_at TEXT,
                    locked_by TEXT REFERENCES users(id),
                    locked_at TEXT,
                    lock_expires_at TEXT,
                    cart_qr_code TEXT,
                    pallet_qr_code TEXT,
                    started_at TEXT
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS exceptions (
                    id TEXT PRIMARY KEY,
                    work_unit_id TEXT NOT NULL REFERENCES work_units(id),
                    order_item_id TEXT NOT NULL REFERENCES order_items(id),
                    type TEXT NOT NULL,
                    quantity REAL NOT NULL,
                    observation TEXT,
                    reported_by TEXT NOT NULL REFERENCES users(id),
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS audit_logs (
                    id TEXT PRIMARY KEY,
                    user_id TEXT REFERENCES users(id),
                    action TEXT NOT NULL,
                    entity_type TEXT NOT NULL,
                    entity_id TEXT,
                    details TEXT,
                    previous_value TEXT,
                    new_value TEXT,
                    ip_address TEXT,
                    user_agent TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            
            # Additional tables for server functionality
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS section_groups (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    sections TEXT NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS picking_sessions (
                    id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL REFERENCES users(id),
                    order_id TEXT NOT NULL REFERENCES orders(id),
                    section_id TEXT NOT NULL,
                    last_heartbeat TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    UNIQUE(order_id, section_id)
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    id TEXT PRIMARY KEY,
                    user_id TEXT NOT NULL REFERENCES users(id),
                    token TEXT NOT NULL,
                    session_key TEXT NOT NULL,
                    expires_at TEXT NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS manual_qty_rules (
                    id TEXT PRIMARY KEY,
                    rule_type TEXT NOT NULL,
                    value TEXT NOT NULL,
                    description TEXT,
                    active INTEGER DEFAULT 1,
                    created_by TEXT REFERENCES users(id),
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS db2_mappings (
                    id TEXT PRIMARY KEY,
                    dataset TEXT NOT NULL,
                    version INTEGER DEFAULT 1 NOT NULL,
                    is_active INTEGER DEFAULT 0 NOT NULL,
                    mapping_json TEXT NOT NULL,
                    description TEXT,
                    created_by TEXT REFERENCES users(id),
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP NOT NULL
                )
            """)
            
            conn.commit()
            log(f"SQLite OK | arquivo=database.db | schema=OK")
        except Exception as e:
            log(f"Erro ao criar tabelas da aplicacao: {e}")
            import traceback
            traceback.print_exc()
            
    except Exception as e:
        log(f"Erro CRITICO ao inicializar SQLite: {e}")
        return

    finally:
        try:
            conn.close()
        except:
            pass


def gerar_sql_orcamentos() -> str:
    """Lê SQL de orçamentos do arquivo .sql"""
    try:
        path_sql = os.path.join(PROJECT_ROOT, "sql", "orcamentos.sql")
        # Check if file exists, if not use fallback
        if not os.path.exists(path_sql):
             log("WARN: sql/orcamentos.sql nao encontrado. Usando query fallback.")
             return "SELECT * FROM DUMMY" # Should not happen if environment is correct
             
        with open(path_sql, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        log(f"Erro ao ler sql/orcamentos.sql: {e}")
        return ""


# (Original query removed)



def gerar_sql_pendentes() -> str:
    """Gera SQL de vendas pendentes (a faturar) - orçamentos com pré-nota não paga, por empresa e vendedor."""
    return """
WITH OrcamentosPendentes AS (
    SELECT
        O.IDORCAMENTO,
        O.IDEMPRESA,
        O.IDCLIFOR AS IDCLIENTE,
        O.DTMOVIMENTO,
        O.DTVALIDADE
    FROM DBA.ORCAMENTO O
    LEFT JOIN DBA.ORCAMENTO_PRE_NOTA OPN
        ON  OPN.IDORCAMENTO        = O.IDORCAMENTO
        AND OPN.IDEMPRESAORCAMENTO = O.IDEMPRESA
    WHERE
        O.FLAGPRENOTA = 'T'
        AND O.FLAGPRENOTAPAGA = 'F'
        AND O.FLAGCANCELADO = 'F'
        AND O.DTMOVIMENTO >= (CURRENT DATE - 2 DAYS)
        AND DATE(COALESCE(O.DTVALIDADE, CURRENT DATE)) >= CURRENT DATE
        AND O.IDEMPRESA IN (1, 3)
        AND COALESCE(OPN.IDPLANILHAPRENOTA, 0) = 0
),
ProdutosPendentesAgregados AS (
    SELECT
        OP.IDEMPRESA,
        OP.IDVENDEDOR,
        OP.IDORCAMENTO,
        SUM(OP.VALTOTLIQUIDO) AS VALOR_TOTAL_ORCAMENTO
    FROM DBA.ORCAMENTO_PROD OP
    INNER JOIN OrcamentosPendentes OPend
        ON  OP.IDORCAMENTO = OPend.IDORCAMENTO
        AND OP.IDEMPRESA   = OPend.IDEMPRESA
    WHERE
        OP.IDVENDEDOR IS NOT NULL
        AND OP.IDVENDEDOR > 0
    GROUP BY
        OP.IDEMPRESA,
        OP.IDVENDEDOR,
        OP.IDORCAMENTO
)
SELECT
    PPA.IDEMPRESA,
    PPA.IDVENDEDOR AS CODIGO_VENDEDOR,
    VEN.NOME AS NOME_VENDEDOR,
    PPA.VALOR_TOTAL_ORCAMENTO AS VALOR_TOTAL
FROM ProdutosPendentesAgregados PPA
LEFT JOIN DBA.CLIENTE_FORNECEDOR VEN
    ON VEN.IDCLIFOR = PPA.IDVENDEDOR
WHERE
    PPA.IDVENDEDOR IN (13656, 1000024, 1005676, 1006781, 1011021, 1000023, 1000020, 1014430)
ORDER BY
    PPA.VALOR_TOTAL_ORCAMENTO DESC
FOR READ ONLY
"""


def gerar_sql_tubos_conexoes() -> str:
    """Gera SQL de tubos e conexões - último 1 ano automaticamente."""
    return """
WITH VendedoresAlvo AS (
    SELECT
        IDCLIFOR,
        NOME
    FROM
        DBA.CLIENTE_FORNECEDOR
    WHERE
        IDCLIFOR IN (13656, 1000024, 1005676, 1006781, 1011021, 1000023, 1000020, 1014430)
),
VendasBrutasPeriodoDetalhe AS (
    SELECT
        EA.IDEMPRESA,
        EA.DTMOVIMENTO,
        EA.IDVENDEDOR,
        CF.NOME AS NomeVendedor,
        CASE
            WHEN OI.TIPOMOVIMENTO = 'E' THEN EA.VALTOTLIQUIDO * -1
            ELSE EA.VALTOTLIQUIDO
        END AS VALOR_LIQUIDO,
        CASE
            WHEN LEFT(COALESCE(P.DESCRCOMPRODUTO, '') || ' ' || COALESCE(PG.SUBDESCRICAO, ''), 4) = 'TUBO'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || ' ' || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%EXTENS.%'
            THEN 'Tubo'
            WHEN LEFT(COALESCE(P.DESCRCOMPRODUTO, '') || ' ' || COALESCE(PG.SUBDESCRICAO, ''), 4) <> 'TUBO'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || ' ' || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%EXTENS.%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%ADESIVO%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%BOIA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%FITA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%4X2%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%VEDACAO%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%CAIXA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%ESFE%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%LAVAT.%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%ELETROD.%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%QUADRO%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%VALVULA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%ENGATE%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%TAMPA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%TORN.%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%FIXACAO%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%RALO%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%GRELHA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%TELHA%'
                 AND (COALESCE(P.DESCRCOMPRODUTO, '') || COALESCE(PG.SUBDESCRICAO, '')) NOT LIKE '%CUMEEIRA%'
            THEN 'Conexao'
            ELSE 'Outros'
        END AS TipoProduto
    FROM
        DBA.ESTOQUE_ANALITICO EA
    INNER JOIN DBA.PRODUTO P
        ON EA.IDPRODUTO = P.IDPRODUTO
    INNER JOIN DBA.PRODUTO_GRADE PG
        ON P.IDPRODUTO = PG.IDPRODUTO AND EA.IDSUBPRODUTO = PG.IDSUBPRODUTO
    INNER JOIN DBA.OPERACAO_INTERNA OI
        ON EA.IDOPERACAO = OI.IDOPERACAO
    LEFT JOIN DBA.CLIENTE_FORNECEDOR CF
        ON EA.IDVENDEDOR = CF.IDCLIFOR
    WHERE
        P.FABRICANTE IN ('AMANCO', 'PLASTUBOS', 'KRONA', 'PRECON')
        AND OI.TIPOMOVIMENTO IN ('V', 'E')
        AND EA.DTMOVIMENTO BETWEEN DATE(SUBSTR(CHAR(CURRENT DATE - 1 YEAR), 1, 7) || '-01') AND CURRENT DATE
        AND EA.IDEMPRESA IN (1, 3)
        AND EA.IDVENDEDOR IN (SELECT IDCLIFOR FROM VendedoresAlvo)
        AND EA.IDVENDEDOR IS NOT NULL
        AND EA.IDVENDEDOR > 0
)
SELECT
    IDEMPRESA,
    DTMOVIMENTO,
    IDVENDEDOR,
    NomeVendedor,
    VALOR_LIQUIDO,
    TipoProduto
FROM
    VendasBrutasPeriodoDetalhe
WHERE
    TipoProduto IN ('Tubo', 'Conexao')
FOR READ ONLY
"""


def sync_orcamentos(conn_db2, conn_sqlite: sqlite3.Connection):
    """Sincroniza tabela cache_orcamentos (Janela 31 dias)."""
    cursor = conn_sqlite.cursor()
    
    # log(f"Sincronizando ORCAMENTOS (Últimos 31 dias)...")
    query = gerar_sql_orcamentos()

    
    try:
        dados = executar_sql_db2(conn_db2, query)
    except Exception as e:
        log(f"  ERRO ao executar query: {e}")
        return
    
    # log(f"  {len(dados)} registros obtidos do DB2")
    
    # ESTRATÉGIA WINDOW SYNC:
    # 1. Deletar tudo da janela (32 dias pra trás para garantir)
    # 2. Inserir tudo novo
    # Isso garante que registros excluídos no DB2 sumam do SQLite.
    
    cutoff_date = (datetime.now() - timedelta(days=32)).strftime('%Y-%m-%d')
    deleted_count = 0
    try:
        cursor.execute("DELETE FROM cache_orcamentos WHERE DTMOVIMENTO >= ?", (cutoff_date,))
        deleted_count = cursor.rowcount
        # log(f"  {deleted_count} registros removidos da janela local (>= {cutoff_date})")
    except Exception as e:
        log(f"  Erro ao limpar janela local: {e}")
    
    inseridos = 0
    erros = 0
    
    for row in dados:
        try:
            # Gera chave única: EMPRESA-ORC-PROD-SUBPROD-SEQ
            chave = f"{row.get('IDEMPRESA')}-{row.get('IDORCAMENTO')}-{row.get('IDPRODUTO')}-{row.get('IDSUBPRODUTO')}-{row.get('NUMSEQUENCIA')}"
            
            cursor.execute("""
                INSERT INTO cache_orcamentos (
                    CHAVE, IDEMPRESA, IDORCAMENTO, IDPRODUTO, IDSUBPRODUTO, NUMSEQUENCIA,
                    QTDPRODUTO, UNIDADE, FABRICANTE, VALUNITBRUTO, VALTOTLIQUIDO, DESCRRESPRODUTO,
                    IDVENDEDOR, IDLOCALRETIRADA, IDSECAO, DESCRSECAO,
                    TIPOENTREGA, NOMEVENDEDOR, TIPOENTREGA_DESCR, LOCALRETESTOQUE,
                    FLAGCANCELADO, IDCLIFOR, DESCLIENTE, DTMOVIMENTO,
                    IDRECEBIMENTO, DESCRRECEBIMENTO, FLAGPRENOTAPAGA,
                    CODBARRAS, CODBARRAS_CAIXA
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                chave,
                int(row.get('IDEMPRESA', 0)),
                int(row.get('IDORCAMENTO', 0)),
                str(row.get('IDPRODUTO', '')),
                str(row.get('IDSUBPRODUTO', '')),
                int(row.get('NUMSEQUENCIA', 0)),
                float(row.get('QTDPRODUTO', 0) or 0),
                str(row.get('UNIDADE', 'UN') or 'UN'),
                str(row.get('FABRICANTE', '') or ''), # Capture FABRICANTE
                float(row.get('VALUNITBRUTO', 0) or 0),
                float(row.get('VALTOTLIQUIDO', 0) or 0),
                row.get('DESCRRESPRODUTO', ''),
                str(row.get('IDVENDEDOR', '')),
                int(row.get('IDLOCALRETIRADA', 0) or 0),
                int(row.get('IDSECAO', 0) or 0),
                row.get('DESCRSECAO', ''),
                row.get('TIPOENTREGA', ''),
                row.get('NOMEVENDEDOR', ''),
                row.get('TIPOENTREGA_DESCR', ''),
                row.get('LOCALRETESTOQUE', ''),
                row.get('FLAGCANCELADO', ''),
                str(row.get('IDCLIFOR', '')),
                row.get('DESCLIENTE', ''),
                formatar_datetime(row.get('DTMOVIMENTO')),
                str(row.get('IDRECEBIMENTO', '')),
                row.get('DESCRRECEBIMENTO', ''),
                row.get('FLAGPRENOTAPAGA', ''),
                str(row.get('CODBARRAS', '') or ''),
                str(row.get('CODBARRAS_CAIXA', '') or '')
            ))
            inseridos += 1
                
        except Exception as e:
            log(f"  Erro ao inserir registro {chave}: {e}")
            erros += 1
    
    conn_sqlite.commit()
    conn_sqlite.commit()
    log(f"ORCAMENTOS (31d) | obtidos={len(dados)} | removidos={deleted_count} (>= {cutoff_date}) | inseridos={inseridos} | erros={erros}")


import uuid

def load_pg_mappings(dataset: str) -> Optional[list]:
    """Carrega mapeamento ativo do PostgreSQL para o dataset especificado."""
    if not psycopg2:
        # log("psycopg2 nao instalado - usando mapeamento legado")
        return None
    
    pg_url = os.environ.get('DATABASE_URL')
    if not pg_url:
        # log("DATABASE_URL nao configurada - usando mapeamento legado")
        return None
    
    try:
        conn_pg = psycopg2.connect(pg_url)
        cursor = conn_pg.cursor()
        cursor.execute(
            "SELECT mapping_json FROM db2_mappings WHERE dataset = %s AND is_active = true ORDER BY version DESC LIMIT 1",
            (dataset,)
        )
        row = cursor.fetchone()
        conn_pg.close()
        
        if row:
            mapping = row[0]
            if isinstance(mapping, str):
                mapping = json_module.loads(mapping)
            # log(f"  Mapping ativo encontrado para '{dataset}' ({len(mapping)} campos)")
            return mapping
        else:
            # log(f"  Nenhum mapping ativo para '{dataset}' - usando mapeamento legado")
            return None
    except Exception as e:
        log(f"  Erro ao carregar mapping do PostgreSQL: {e}")
        return None


def apply_mapping(row: dict, mapping: list) -> dict:
    """Aplica um mapeamento a uma linha de dados do cache."""
    result = {}
    for field_map in mapping:
        app_field = field_map.get('appField', '')
        db_expr = field_map.get('dbExpression', '')
        cast = field_map.get('cast', '')
        default = field_map.get('defaultValue', '')
        
        value = None
        if db_expr:
            value = row.get(db_expr)
            if value is None:
                value = row.get(db_expr.upper())
            if value is None:
                value = row.get(db_expr.lower())
        
        if value is None or value == '':
            value = default if default else None
        
        if value is not None and cast:
            try:
                if cast == 'number':
                    value = float(value)
                elif cast == 'string':
                    value = str(value)
                elif cast == 'divide_100':
                    value = float(value) / 100.0
                elif cast == 'divide_1000':
                    value = float(value) / 1000.0
                elif cast == 'boolean_T_F':
                    value = (str(value).upper() == 'T')
            except (ValueError, TypeError):
                pass
        
        result[app_field] = value
    return result


def transform_data(conn_sqlite: sqlite3.Connection):
    """
    Transforma dados brutos de cache_orcamentos em orders/products/work_units
    para uso da aplicação. Otimizado com Bulk Insert.
    """
    # Carregar mapeamentos do PostgreSQL (se disponiveis)
    orders_mapping = load_pg_mappings("orders")
    products_mapping = load_pg_mappings("products")
    items_mapping = load_pg_mappings("order_items")
    
    use_dynamic_mapping = (orders_mapping is not None or products_mapping is not None or items_mapping is not None)

    if use_dynamic_mapping:
        log("Transformação | Usando mapeamento dinâmico do Mapping Studio")
    # else:
    #     log("Transformação | WARN: psycopg2 ausente; usando mapeamento legado (hardcoded)")
    
    cursor = conn_sqlite.cursor()
    
    # 1. Obter dados do cache
    cursor.execute("SELECT * FROM cache_orcamentos")
    rows = cursor.fetchall()
    
    if not rows:
        return
        
    col_names = [description[0] for description in cursor.description]
    
    # Pre-loading Existing Data IDs to memory for fast lookup
    # Sets are O(1)
    cursor.execute("SELECT erp_order_id, id FROM orders")
    existing_orders = {r[0]: r[1] for r in cursor.fetchall()}
    
    cursor.execute("SELECT erp_code, id FROM products")
    existing_products = {r[0]: r[1] for r in cursor.fetchall()}
    
    # For items, we need to know if (order_id, product_id) exists.
    # We map (order_uuid, product_uuid) -> item_id
    cursor.execute("SELECT order_id, product_id FROM order_items")
    existing_items = set((r[0], r[1]) for r in cursor.fetchall()) # Set of tuples
    
    # Work units
    cursor.execute("SELECT order_id, section, pickup_point FROM work_units")
    existing_work_units = set((r[0], str(r[1]) if r[1] is not None else None, int(r[2]) if r[2] is not None else 0) for r in cursor.fetchall())
    
    # DEBUG logs removed to reduce console spam
    # if not QUIET:
    #     print(f"DEBUG: existing_items size: {len(existing_items)}")
    #     print(f"DEBUG: Sample existing_items: {list(existing_items)[:5]}")

    
    # Batches for insert
    upsert_orders = []
    new_products = []
    new_items = []
    unique_pickup_points = set()
    unique_sections = set()
    new_work_units = []
    
    # Helper Data Structures for this Batch
    # erp_code -> uuid (for things created in this batch)
    batch_products_map = {} 
    
    orders_map = {} # erp_order_id -> {total, items: [], ...}

    # Pass 1: Aggregate Rows into Orders in Memory
    for row_tuple in rows:
        row = dict(zip(col_names, row_tuple))
        
        # Use dynamic mapping if available, otherwise use legacy hardcoded mapping
        if orders_mapping:
            mapped_order = apply_mapping(row, orders_mapping)
            id_empresa = str(row.get('IDEMPRESA', ''))
            erp_order_id = str(mapped_order.get('erp_order_id', ''))
            map_key = f"{id_empresa}-{erp_order_id}"
            
            if map_key not in orders_map:
                orders_map[map_key] = {
                    'erp_id_display': erp_order_id,
                    'customer_name': mapped_order.get('customer_name') or 'Cliente Desconhecido',
                    'customer_code': str(mapped_order.get('customer_code') or ''),
                    'total_value': 0.0,
                    'items': [],
                    'created_at': mapped_order.get('created_at'),
                    'pickup_point': mapped_order.get('pickup_point'),
                    'section': mapped_order.get('section'),
                    'flag_pre_nota_paga': None,  # Handled by financial_status mapping
                    'financial_status': mapped_order.get('financial_status'),
                }
            
            val_liq = float(mapped_order.get('total_value') or 0)
            orders_map[map_key]['total_value'] += val_liq
            orders_map[map_key]['items'].append(row)
        else:
            # Legacy hardcoded mapping
            id_empresa = str(row.get('IDEMPRESA'))
            id_orcamento = str(row.get('IDORCAMENTO'))
            erp_order_id = id_orcamento
            map_key = f"{id_empresa}-{id_orcamento}"
            
            if map_key not in orders_map:
                orders_map[map_key] = {
                    'erp_id_display': erp_order_id,
                    'customer_name': row.get('DESCLIENTE') or 'Cliente Desconhecido',
                    'customer_code': str(row.get('IDCLIFOR') or ''),
                    'total_value': 0.0,
                    'items': [],
                    'created_at': row.get('DTMOVIMENTO'),
                    'pickup_point': row.get('IDLOCALRETIRADA'),
                    'section': row.get('IDSECAO'),
                    'flag_pre_nota_paga': row.get('FLAGPRENOTAPAGA')
                }
            
            val_liq = float(row.get('VALTOTLIQUIDO') or 0) / 100.0
            orders_map[map_key]['total_value'] += val_liq
            orders_map[map_key]['items'].append(row)

    # Pass 2: Process Aggregated Orders
    for map_key, data in orders_map.items():
        
        erp_order_id = data['erp_id_display']
        
        # --- ORDER ---
        # Note: erp_order_id (IDORCAMENTO) needs to be unique in `orders` table.
        # If we have same ID in different companies, this might crash schema unique constraint.
        # But we only sync Company 3, so it is fine.
        order_uuid = existing_orders.get(erp_order_id)
        if not order_uuid:
            order_uuid = str(uuid.uuid4())
            existing_orders[erp_order_id] = order_uuid # Add to separate lookups if needed later?
            
        # Map Financial Status
        if data.get('financial_status'):
            fin_status = data['financial_status']
        elif data.get('flag_pre_nota_paga') == 'T':
            fin_status = 'faturado'
        else:
            fin_status = 'pendente'

        # Prepare pickup_points JSON
        pickup_point_val = data.get('pickup_point')
        pickup_points_json = json.dumps([pickup_point_val]) if pickup_point_val else '[]'

        # Always add to upsert list (Update existing ones too)
        upsert_orders.append((
            order_uuid, erp_order_id, data['customer_name'], data['customer_code'], 
            data['total_value'], fin_status, pickup_points_json, data.get('created_at')
        ))
        
        # --- WORK UNIT (Removed: Will be calculated from items) ---
        # if order_uuid not in existing_work_units:
        #      new_work_units.append((
        #          str(uuid.uuid4()), order_uuid, data.get('pickup_point') or 0, None
        #      ))
        #      existing_work_units.add(order_uuid)

        # Track sections/pickup_points for this order
        order_distinct_configs = set()

             
        # --- ITEMS ---
        for item in data['items']:
            if products_mapping and items_mapping:
                mapped_prod = apply_mapping(item, products_mapping)
                mapped_item = apply_mapping(item, items_mapping)
                erp_prod_code = str(mapped_prod.get('erp_code') or mapped_item.get('erp_product_code') or '')
                prod_uuid = existing_products.get(erp_prod_code)
                unit = str(mapped_prod.get('unit') or 'UN')
                manufacturer = str(mapped_prod.get('manufacturer') or '')
                real_qty = float(mapped_item.get('quantity') or 0)
                
                if not prod_uuid:
                    prod_uuid = batch_products_map.get(erp_prod_code)
                    if not prod_uuid:
                        prod_uuid = str(uuid.uuid4())
                        new_products.append((
                            prod_uuid, erp_prod_code,
                            mapped_prod.get('barcode'), mapped_prod.get('box_barcode'),
                            mapped_prod.get('name'),
                            str(mapped_prod.get('section') or ''), mapped_prod.get('pickup_point'),
                            unit, manufacturer,
                            mapped_prod.get('price')
                        ))
                        batch_products_map[erp_prod_code] = prod_uuid
            else:
                # Legacy hardcoded mapping
                erp_prod_code = str(item.get('IDPRODUTO'))
                prod_uuid = existing_products.get(erp_prod_code)
                unit = item.get('UNIDADE') or 'UN'
                manufacturer = item.get('FABRICANTE') or ''
                raw_qty = float(item.get('QTDPRODUTO') or 0)
                real_qty = raw_qty / 1000.0
                


                if not prod_uuid:
                    prod_uuid = batch_products_map.get(erp_prod_code)
                    if not prod_uuid:
                        prod_uuid = str(uuid.uuid4())
                        new_products.append((
                            prod_uuid, erp_prod_code, item.get('CODBARRAS'), item.get('CODBARRAS_CAIXA'), item.get('DESCRRESPRODUTO'), 
                            str(item.get('IDSECAO')), item.get('IDLOCALRETIRADA'),
                            unit, 
                            manufacturer,
                            item.get('VALUNITBRUTO')
                        ))
                        batch_products_map[erp_prod_code] = prod_uuid
            
            # Determine Pickup Point & Section
            if items_mapping:
                mapped_item_data = apply_mapping(item, items_mapping)
                item_pickup = mapped_item_data.get('pickup_point')
                item_section = str(mapped_item_data.get('section') or '')
            else:
                item_pickup = item.get('IDLOCALRETIRADA')
                item_section = str(item.get('IDSECAO'))
            
            # Capture Pickup Point Name (Always, even if item exists)
            if item_pickup and item_pickup > 0:
                pp_name = item.get('LOCALRETESTOQUE') or f"Ponto {item_pickup}"
                unique_pickup_points.add((item_pickup, pp_name))

            # Capture Section Name
            if item_section and str(item_section).isdigit():
                sec_id = int(item_section)
                sec_name = item.get('DESCRSECAO') or f"Seção {sec_id}"
                unique_sections.add((sec_id, sec_name))

            # Item Relation
            if (order_uuid, prod_uuid) not in existing_items:
                new_items.append((
                    str(uuid.uuid4()), order_uuid, prod_uuid, real_qty,
                    item_pickup, item_section
                ))
                existing_items.add((order_uuid, prod_uuid))

            # Add to configs for Work Units
            # Use '0' string for section if empty, or handle as None?
            # DB stores TEXT for section. We preserve what came from item logic.
            # For pickup, ensure int
            wu_section = item_section if item_section else None
            wu_pickup = int(item_pickup) if item_pickup else 0
            order_distinct_configs.add((wu_section, wu_pickup))

        # --- Create Work Units based on Distinct Items ---
        for (sec, pp) in order_distinct_configs:
            # Check existence: order_id + section + pickup
            # Note: existing_work_units now stores (order_id, section, pickup)
            # Ensure types match for lookup
            lookup_sec = str(sec) if sec is not None else None
            lookup_pp = int(pp)
            
            if (order_uuid, lookup_sec, lookup_pp) not in existing_work_units:
                new_work_units.append((
                    str(uuid.uuid4()), order_uuid, pp, sec
                ))
                # Add to set to avoid dups in this same run (if any)
                existing_work_units.add((order_uuid, lookup_sec, lookup_pp))



    # Insert Pickup Points
    try:
        if unique_pickup_points:
            cursor.executemany("INSERT OR REPLACE INTO pickup_points (id, name, active) VALUES (?, ?, 1)", list(unique_pickup_points))
            
        if unique_sections:
            cursor.executemany("INSERT OR REPLACE INTO sections (id, name) VALUES (?, ?)", list(unique_sections))
            
        conn_sqlite.commit()
    except Exception as e:
        log(f"Erro ao inserir pontos/seções: {e}")

    # 3. Bulk Inserts
    try:
        if new_products:
            # Update INSERT to use dynamic unit
            cursor.executemany("""
                INSERT OR IGNORE INTO products (id, erp_code, barcode, box_barcode, name, section, pickup_point, unit, manufacturer, price)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, new_products)
            
        if upsert_orders:
            # Upsert Logic: Update Financial Status if order exists
            cursor.executemany("""
                INSERT INTO orders (id, erp_order_id, customer_name, customer_code, total_value, financial_status, pickup_points, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, 'pendente', ?)
                ON CONFLICT(erp_order_id) DO UPDATE SET
                    financial_status = excluded.financial_status,
                    total_value = excluded.total_value,
                    customer_name = excluded.customer_name,
                    pickup_points = excluded.pickup_points,
                    updated_at = CURRENT_TIMESTAMP
            """, upsert_orders)
            
        if new_items:
            # Note: order_items usually doesn't have unique constraint on (order,product) in schema unless we added it?
            # Schema says: id is PK. No unique on pair.
            # But we filtered using existing_items set, so we are safe from duplication against DB.
            cursor.executemany("""
                INSERT INTO order_items (id, order_id, product_id, quantity, separated_qty, status, pickup_point, section)
                VALUES (?, ?, ?, ?, 0, 'pendente', ?, ?)
            """, new_items)
            
        if new_work_units:
            cursor.executemany("""
                INSERT INTO work_units (id, order_id, status, type, pickup_point, section)
                VALUES (?, ?, 'pendente', 'separacao', ?, ?)
            """, new_work_units)

        conn_sqlite.commit()
        
        # Log Summary
        log(f"Transformação | pedidos_processados={len(orders_map)} | pedidos_upsert={len(upsert_orders)} | novos_itens={len(new_items)}")
        
    except Exception as e:
        log(f"Erro no Bulk Insert: {e}")
        import traceback
        traceback.print_exc()

def kill_port_411():
    """Mata processo usando a porta 411 (Windows) para evitar EADDRINUSE."""
    if sys.platform == "win32":
        try:
            # Encontrar PID
            result = subprocess.run('netstat -ano | findstr :411', shell=True, capture_output=True, text=True)
            output = result.stdout.strip()
            if output:
                lines = output.split('\n')
                for line in lines:
                    parts = line.split()
                    if len(parts) >= 5:
                        pid = parts[-1] 
                        if pid != '0':
                            log(f"Porta 411 em uso pelo PID {pid}. Matando...")
                            subprocess.run(f'taskkill /F /PID {pid}', shell=True, capture_output=True)
        except Exception as e:
            log(f"Aviso: Não foi possível limpar porta 411: {e}")

def sync_pendentes(conn_db2, conn_sqlite: sqlite3.Connection):
    """Sincroniza tabela cache_vendas_pendentes (sempre substitui)."""
    cursor = conn_sqlite.cursor()
    
    log("Sincronizando VENDAS_PENDENTES (substituindo dados atuais)...")
    
    try:
        dados = executar_sql_db2(conn_db2, gerar_sql_pendentes())
    except Exception as e:
        log(f"  ERRO ao executar query: {e}")
        return
    
    log(f"  {len(dados)} registros obtidos do DB2")
    
    cursor.execute("DELETE FROM cache_vendas_pendentes")
    
    inseridos = 0
    for row in dados:
        try:
            valor_total = float(row.get('VALOR_TOTAL', 0) or 0) / 100
            cursor.execute("""
                INSERT INTO cache_vendas_pendentes (IDEMPRESA, CODIGO_VENDEDOR, NOME_VENDEDOR, VALOR_TOTAL)
                VALUES (?, ?, ?, ?)
            """, (
                int(row.get('IDEMPRESA', 1) or 1),
                str(row.get('CODIGO_VENDEDOR', '')),
                row.get('NOME_VENDEDOR', ''),
                valor_total,
            ))
            inseridos += 1
            log(f"    Vendedor {row.get('NOME_VENDEDOR', '')}: R$ {valor_total:,.2f}")
        except Exception as e:
            log(f"  Erro ao inserir: {e}")
    
    conn_sqlite.commit()
    log(f"  {inseridos} registros salvos em cache_vendas_pendentes")


def sync_tubos_conexoes(conn_db2, conn_sqlite: sqlite3.Connection):
    """Sincroniza tabela cache_tubos_conexoes (sempre substitui - último 1 ano)."""
    cursor = conn_sqlite.cursor()
    
    log("Sincronizando TUBOS_CONEXOES (último 1 ano)...")
    
    try:
        dados = executar_sql_db2(conn_db2, gerar_sql_tubos_conexoes())
    except Exception as e:
        log(f"  ERRO ao executar query: {e}")
        return
    
    log(f"  {len(dados)} registros obtidos do DB2")
    
    cursor.execute("DELETE FROM cache_tubos_conexoes")
    
    inseridos = 0
    for row in dados:
        try:
            cursor.execute("""
                INSERT INTO cache_tubos_conexoes (IDEMPRESA, DT_MOVIMENTO, IDVENDEDOR, NOME_VENDEDOR, VALOR_LIQUIDO, TIPO_PRODUTO)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                int(row.get('IDEMPRESA', 1) or 1),
                formatar_data(row.get('DTMOVIMENTO')),
                str(row.get('IDVENDEDOR', '')),
                row.get('NomeVendedor', '') or row.get('NOMEVENDEDOR', ''),
                float(row.get('VALOR_LIQUIDO', 0) or 0) / 1000000,
                row.get('TipoProduto', '') or row.get('TIPOPRODUTO', ''),
            ))
            inseridos += 1
        except Exception as e:
            log(f"  Erro ao inserir: {e}")
    
    conn_sqlite.commit()
    log(f"  {inseridos} registros salvos em cache_tubos_conexoes")


def sincronizar(data_inicial: Optional[str] = None) -> bool:
    """Fluxo principal de sincronização."""
    inicio = time.time()
    
    
    # Validacao do driver ja feita no import
    # if not pyodbc: ...

    try:
        conn_db2 = conectar_db2()
    except Exception as e:
        log(f"ERRO FATAL DB2: {e}")
        return False
        
    try:
        conn_sqlite = sqlite3.connect(DATABASE_PATH)
        
        sync_orcamentos(conn_db2, conn_sqlite)
        transform_data(conn_sqlite)
        
    # Vendas Pendentes e Tubos foram removidos do fluxo.
    # sync_pendentes(conn_db2, conn_sqlite)
    # sync_tubos_conexoes(conn_db2, conn_sqlite)
        
    # Vendas Pendentes e Tubos foram removidos do fluxo.
    # sync_pendentes(conn_db2, conn_sqlite)
    # sync_tubos_conexoes(conn_db2, conn_sqlite)
        
        duracao = time.time() - inicio
        duracao = time.time() - inicio
        # log(f"Sync concluído | duração={duracao:.2f}s")
        
        return True
        
        return True
    except Exception as e:
        log(f"ERRO NO PROCESSO DE SYNC: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        try:
            conn_db2.close()
            conn_sqlite.close()
        except:
            pass


def iniciar_servidor():
    """Inicia o servidor web do dashboard."""
    log("Iniciando servidor web...")
    os.chdir(PROJECT_ROOT)
    
    is_windows = sys.platform == "win32"
    
    try:
        subprocess.run("npm --version", shell=True, capture_output=True, check=True)
    except (FileNotFoundError, subprocess.CalledProcessError):
        log("ERRO: npm não encontrado. Instale Node.js primeiro.")
        log("Baixe em: https://nodejs.org/")
        return
    
    log(f"Acesse: http://localhost:411")
    
    # Auto-fix port
    kill_port_411()
    
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Web OK | url=http://localhost:411")
    
    if is_windows:
        # log("Executando servidor (Windows)...")
        env = os.environ.copy()
        env["NODE_ENV"] = "development"
        env["PORT"] = "411"
        try:
            subprocess.run("npx tsx server/index.ts", shell=True, cwd=PROJECT_ROOT, env=env)
        except KeyboardInterrupt:
             log("\nServidor interrompido pelo usuário.")
    else:
        log("Executando: npm run dev")
        env = os.environ.copy()
        env["PORT"] = "411"
        try:
            subprocess.run("npm run dev", shell=True, cwd=PROJECT_ROOT, env=env)
        except KeyboardInterrupt:
            log("\nServidor interrompido pelo usuário.")


def loop_sync(intervalo: int, data_inicial: Optional[str] = None):
    """Executa sync em loop (para rodar em thread separada)."""
    while True:
        time.sleep(intervalo)
        log(f"Sincronização incremental automática...")
        sincronizar(data_inicial=None)  # Incremental após a primeira
        log(f"Próxima sync em {intervalo} segundos ({intervalo//60} min)")


def main():
    global QUIET
    parser = argparse.ArgumentParser(
        description="Sincronizador DB2 -> SQLite",
        epilog="""
Exemplos:
  python sync_db2.py --serve          # Sync + Servidor (Loop padrão 5 min)
  python sync_db2.py --loop 600       # Apenas Sync (Loop 10 min)
        """
    )
    parser.add_argument("--desde", type=str, metavar="YYYY-MM-DD",
                        help="Ignorado nesta versão (Janela fixa 31 dias)")
    parser.add_argument("--loop", type=int, metavar="SEGUNDOS",
                        help="Intervalo do loop (padrão 300s = 5min)")
    parser.add_argument("--serve", action="store_true",
                        help="Inicia o servidor web após sync")
    parser.add_argument("--quiet", action="store_true",
                        help="Suprime logs no stdout")
    
    args = parser.parse_args()

    if args.quiet:
        QUIET = True
    
    # 1. Sincronização Inicial (Bloqueante)
    # Ex: [2026-02-08 21:30:13] Sync iniciado | modo=serve | SO=Windows
    modo_str = "serve" if args.serve else ("loop" if args.loop else "once")
    if not QUIET:
        log(f"Sync iniciado | modo={modo_str} | SO={platform.system()}")

    # Garantir que tabelas existam
    inicializar_sqlite()
    
    # Passar args para sincronizar
    sucesso = sincronizar(data_inicial=args.desde)
    
    # 2. Configurar Loop (Thread se Serve, Main se Loop-Only)
    should_loop = args.loop is not None or args.serve
    intervalo = args.loop if args.loop else 300

    if should_loop:
        if args.loop or args.serve:
             # Se foi explicito o loop ou tem server (que implica loop default), avisa
             # Mas se for so server sem args.loop, o usuario nao pediu explicito o loop, mas o sistema faz.
             # O log original do usuario nao mostrava "Modo Loop ativado" se nao passasse --loop? 
             # O usuario pediu output exato.
             # [2026-02-08 21:51:09] Sync concluído | duração=8.13s
             # [2026-02-08 21:30:24] Web OK ...
             # Nao tem log de "Modo Loop" no exemplo dele.
             if args.loop:
                 log(f"Modo Loop ativado: {intervalo} segundos")
        
        def loop_sync_internal(): 
            while True:
                time.sleep(intervalo)
                sincronizar()
        
        if args.serve:
            # Thread para o loop, Main para o servidor
            t = threading.Thread(target=loop_sync_internal, daemon=True)
            t.start()
        else:
            # Main para o loop
            try:
                loop_sync_internal()
            except KeyboardInterrupt:
                if not args.quiet:
                    log("\nLoop interrompido pelo usuário.")

    # 3. Servidor Web
    if args.serve:
        iniciar_servidor()
        # Apenas Uma Execução
        if not sucesso:
            sys.exit(1)
            
if __name__ == "__main__":
    main()
